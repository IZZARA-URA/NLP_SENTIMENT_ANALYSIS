{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e812a42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.legacy import data\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy', tokenizer_language = 'en_core_web_sm')\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a891f832",
   "metadata": {},
   "source": [
    "### IMDB DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25d312ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz: 100% 84.1M/84.1M [13:36<00:00, 103kB/s] \n"
     ]
    }
   ],
   "source": [
    "from torchtext.legacy import datasets\n",
    "\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d2a0de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 17500\n",
      "Number of testing  examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing  examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b213017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['I', 'would', 'not', 'like', 'to', 'comment', 'on', 'how', 'good', 'the', 'movie', 'was', 'or', 'what', 'were', 'the', 'flaws', 'as', 'I', 'am', 'not', 'a', 'professional', 'film', 'critic', 'and', 'I', 'do', 'not', 'have', 'enough', 'knowledge', 'of', 'making', 'movies', '.', 'What', 'i', 'do', 'know', 'is', 'that', 'making', 'this', 'kind', 'of', 'a', 'movie', 'in', 'your', 'very', 'first', 'shot', 'is', 'a', 'big', 'achievement', 'and', 'I', 'would', 'like', 'to', 'congratulate', 'the', 'Director', 'for', 'that', '.', 'However', ',', 'in', 'some', 'reviews', ',', 'that', 'i', 'have', 'read', ',', 'critics', 'have', 'complained', 'that', 'Hiralal', \"'s\", 'relationship', 'with', 'his', 'brothers', 'was', 'not', 'highlighted', ',', 'and', 'his', 'siblings', 'were', 'completely', 'erased', 'from', 'the', 'story', '.', 'Now', 'i', 'would', 'really', 'like', 'to', 'raise', 'a', 'point', 'here', 'that', 'as', 'the', 'name', 'of', 'the', 'movie', 'suggests', ',', 'it', 'is', 'not', 'a', 'movie', 'about', 'Hiralal', \"'s\", 'brothers', ',', 'it', 'is', 'a', 'movie', 'on', 'the', 'relationship', 'of', 'Mahatma', 'Gandhi', 'and', 'his', 'son', 'Hiralal', 'Gandhi', ',', 'nothing', 'more', 'nothing', 'less', '.', 'If', 'we', 'start', 'complaining', 'about', 'some', 'characters', 'being', 'kept', 'out', 'of', 'action', 'in', 'the', 'movie', ',', 'it', 'would', 'be', 'a', 'bit', 'unfair', 'because', 'these', 'characters', 'do', \"n't\", 'fit', 'in', 'the', 'picture', ',', 'no', 'matter', 'how', 'relevant', 'they', 'were', 'in', 'real', 'life', '.', 'So', 'i', 'think', 'it', 'would', 'be', 'better', 'if', 'we', 'stick', 'to', 'the', 'main', 'idea', 'and', 'stop', 'satisfy', 'a', 'critic', 'in', 'ourselves.<br', '/><br', '/>Enjoy', '!', '!', '!', '!'], 'label': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41d5cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47057fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples:   {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples:    {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5549c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c4687b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 25002\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a23222f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 201598), (',', 191166), ('.', 164264), ('a', 108869), ('and', 108688), ('of', 99786), ('to', 92940), ('is', 75592), ('in', 60795), ('I', 53840), ('it', 53227), ('that', 48734), ('\"', 44249), (\"'s\", 43318), ('this', 41980), ('-', 37242), ('/><br', 35396), ('was', 34638), ('as', 30354), ('with', 29730)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f162190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'neg': 0, 'pos': 1})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b47229",
   "metadata": {},
   "source": [
    "### LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59b3f627",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8351c4b4",
   "metadata": {},
   "source": [
    "### MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05295aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dim, \n",
    "        embedding_dim, \n",
    "        hidden_dim, \n",
    "        output_dim\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b35ed6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6309e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,592,105 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c819f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c6c745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8f51dd",
   "metadata": {},
   "source": [
    "### TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb41a2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36374ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41e0b293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.694 | Train Acc: 50.28%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 49.71%\n",
      "Epoch: 02 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.693 | Train Acc: 49.84%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 49.99%\n",
      "Epoch: 03 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.08%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 50.66%\n",
      "Epoch: 04 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.693 | Train Acc: 49.85%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 49.57%\n",
      "Epoch: 05 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.19%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 50.69%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "114f65a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.709 | Test Acc: 47.88%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16f5a6e",
   "metadata": {},
   "source": [
    "# NBoW : Natural Bag-of-Words\n",
    "### model are stromg whem performimg sentiment analysis or text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27214e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import sys\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import tqdm\n",
    "\n",
    "seed = 0\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "train_data, test_data = datasets.load_dataset(\"imdb\", split=[\"train\", \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f343e66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 25000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 25000\n",
       " }))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e191d9f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(num_classes=2, names=['neg', 'pos'], names_file=None, id=None)}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6eeee69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4aaf1859",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f65a6667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'world',\n",
       " '!',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " 'today',\n",
       " '?',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'doing',\n",
       " 'fantastic',\n",
       " '!']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello world! How are you doing today? I'm doing fantastic!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c9402a",
   "metadata": {},
   "source": [
    "### CLEAN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e63e2a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.fingerprint:Parameter 'fn_kwargs'={'tokenizer': <function _basic_english_normalize at 0x7f8117704670>, 'max_length': 256} of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c33e5e83ce41fd82a862341476d6b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf9fc070f394538962a9402dbba4a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'tokens'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_example(example, tokenizer, max_length):\n",
    "    tokens = tokenizer(example['text'])[:max_length]\n",
    "    return {'tokens':tokens}\n",
    "\n",
    "max_length = 256\n",
    "\n",
    "train_data = train_data.map(tokenize_example, fn_kwargs={'tokenizer': tokenizer, \n",
    "                                                         'max_length':max_length})\n",
    "test_data  = test_data.map(tokenize_example,  fn_kwargs={'tokenizer': tokenizer, \n",
    "                                                         'max_length':max_length})\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a4acdfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(num_classes=2, names=['neg', 'pos'], names_file=None, id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c3faf98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell',\n",
       " 'high',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cartoon',\n",
       " 'comedy',\n",
       " '.',\n",
       " 'it',\n",
       " 'ran',\n",
       " 'at',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " 'as',\n",
       " 'some',\n",
       " 'other',\n",
       " 'programs',\n",
       " 'about',\n",
       " 'school',\n",
       " 'life',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'teachers',\n",
       " '.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]['tokens'][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b36aeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.25 \n",
    "train_valid_data = train_data.train_test_split(test_size=test_size)\n",
    "train_data = train_valid_data['train']\n",
    "valid_data = train_valid_data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6cc1079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 5 \n",
    "special_tokens = ['<unk>', '<pad>']\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(train_data['tokens'],\n",
    "                                                 min_freq=min_freq,\n",
    "                                                 specials=special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "35f0602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.get_itos()[:10]\n",
    "unk_index = vocab['<unk>']\n",
    "pad_index  = vocab['<pad>']\n",
    "vocab.set_default_index(unk_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "16074e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_data(example, vocab):\n",
    "    ids = [vocab[token] for token in example['tokens']]\n",
    "    return {'ids': ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dcf0274e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695a74b47418480cadd6c91101a566bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18750 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb1539f61484eab9e3c10b5fde8c6f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18750 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09dd774264ed47f3ac4febd84c5fb9f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18750 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = train_data.map(numericalize_data, fn_kwargs={'vocab': vocab})\n",
    "valid_data = train_data.map(numericalize_data, fn_kwargs={'vocab': vocab})\n",
    "test_data = train_data.map(numericalize_data, fn_kwargs={'vocab': vocab})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e63febb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'This documentary is at its best when it is simply showing the ayurvedic healers\\' offices and treatment preparation. There is no denying the grinding poverty in India and desperation of even their wealthier clients. However, as an argument for ayurvedic medicine in general, this film fails miserably. Although Indian clients mention having seen \"aleopathic\" doctors, those doctors are not interviewed, and we have to take the vague statements of their patients at face value-- \"the doctor said there was no cure,\" \"the doctor said it was cancer\" etc. Well, \"no cure\" doesn\\'t mean \"no treatment,\" and what type of cancer exactly does the patient have? The film is at its most feeble when showing ayurvedic practice in America. There it is reduced, apparently, to the stunning suggestion that having a high powered Wall Street job can make your stomach hurt.',\n",
       " 'label': 0,\n",
       " 'tokens': ['this',\n",
       "  'documentary',\n",
       "  'is',\n",
       "  'at',\n",
       "  'its',\n",
       "  'best',\n",
       "  'when',\n",
       "  'it',\n",
       "  'is',\n",
       "  'simply',\n",
       "  'showing',\n",
       "  'the',\n",
       "  'ayurvedic',\n",
       "  'healers',\n",
       "  \"'\",\n",
       "  'offices',\n",
       "  'and',\n",
       "  'treatment',\n",
       "  'preparation',\n",
       "  '.',\n",
       "  'there',\n",
       "  'is',\n",
       "  'no',\n",
       "  'denying',\n",
       "  'the',\n",
       "  'grinding',\n",
       "  'poverty',\n",
       "  'in',\n",
       "  'india',\n",
       "  'and',\n",
       "  'desperation',\n",
       "  'of',\n",
       "  'even',\n",
       "  'their',\n",
       "  'wealthier',\n",
       "  'clients',\n",
       "  '.',\n",
       "  'however',\n",
       "  ',',\n",
       "  'as',\n",
       "  'an',\n",
       "  'argument',\n",
       "  'for',\n",
       "  'ayurvedic',\n",
       "  'medicine',\n",
       "  'in',\n",
       "  'general',\n",
       "  ',',\n",
       "  'this',\n",
       "  'film',\n",
       "  'fails',\n",
       "  'miserably',\n",
       "  '.',\n",
       "  'although',\n",
       "  'indian',\n",
       "  'clients',\n",
       "  'mention',\n",
       "  'having',\n",
       "  'seen',\n",
       "  'aleopathic',\n",
       "  'doctors',\n",
       "  ',',\n",
       "  'those',\n",
       "  'doctors',\n",
       "  'are',\n",
       "  'not',\n",
       "  'interviewed',\n",
       "  ',',\n",
       "  'and',\n",
       "  'we',\n",
       "  'have',\n",
       "  'to',\n",
       "  'take',\n",
       "  'the',\n",
       "  'vague',\n",
       "  'statements',\n",
       "  'of',\n",
       "  'their',\n",
       "  'patients',\n",
       "  'at',\n",
       "  'face',\n",
       "  'value--',\n",
       "  'the',\n",
       "  'doctor',\n",
       "  'said',\n",
       "  'there',\n",
       "  'was',\n",
       "  'no',\n",
       "  'cure',\n",
       "  ',',\n",
       "  'the',\n",
       "  'doctor',\n",
       "  'said',\n",
       "  'it',\n",
       "  'was',\n",
       "  'cancer',\n",
       "  'etc',\n",
       "  '.',\n",
       "  'well',\n",
       "  ',',\n",
       "  'no',\n",
       "  'cure',\n",
       "  'doesn',\n",
       "  \"'\",\n",
       "  't',\n",
       "  'mean',\n",
       "  'no',\n",
       "  'treatment',\n",
       "  ',',\n",
       "  'and',\n",
       "  'what',\n",
       "  'type',\n",
       "  'of',\n",
       "  'cancer',\n",
       "  'exactly',\n",
       "  'does',\n",
       "  'the',\n",
       "  'patient',\n",
       "  'have',\n",
       "  '?',\n",
       "  'the',\n",
       "  'film',\n",
       "  'is',\n",
       "  'at',\n",
       "  'its',\n",
       "  'most',\n",
       "  'feeble',\n",
       "  'when',\n",
       "  'showing',\n",
       "  'ayurvedic',\n",
       "  'practice',\n",
       "  'in',\n",
       "  'america',\n",
       "  '.',\n",
       "  'there',\n",
       "  'it',\n",
       "  'is',\n",
       "  'reduced',\n",
       "  ',',\n",
       "  'apparently',\n",
       "  ',',\n",
       "  'to',\n",
       "  'the',\n",
       "  'stunning',\n",
       "  'suggestion',\n",
       "  'that',\n",
       "  'having',\n",
       "  'a',\n",
       "  'high',\n",
       "  'powered',\n",
       "  'wall',\n",
       "  'street',\n",
       "  'job',\n",
       "  'can',\n",
       "  'make',\n",
       "  'your',\n",
       "  'stomach',\n",
       "  'hurt',\n",
       "  '.'],\n",
       " 'ids': [14,\n",
       "  627,\n",
       "  10,\n",
       "  37,\n",
       "  100,\n",
       "  125,\n",
       "  60,\n",
       "  11,\n",
       "  10,\n",
       "  361,\n",
       "  834,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  9,\n",
       "  12187,\n",
       "  6,\n",
       "  2407,\n",
       "  9694,\n",
       "  3,\n",
       "  46,\n",
       "  10,\n",
       "  66,\n",
       "  8861,\n",
       "  2,\n",
       "  16732,\n",
       "  3705,\n",
       "  13,\n",
       "  2360,\n",
       "  6,\n",
       "  4374,\n",
       "  7,\n",
       "  69,\n",
       "  77,\n",
       "  0,\n",
       "  13332,\n",
       "  3,\n",
       "  190,\n",
       "  4,\n",
       "  19,\n",
       "  41,\n",
       "  4597,\n",
       "  21,\n",
       "  0,\n",
       "  6574,\n",
       "  13,\n",
       "  822,\n",
       "  4,\n",
       "  14,\n",
       "  23,\n",
       "  962,\n",
       "  3426,\n",
       "  3,\n",
       "  265,\n",
       "  1267,\n",
       "  13332,\n",
       "  798,\n",
       "  266,\n",
       "  111,\n",
       "  0,\n",
       "  5592,\n",
       "  4,\n",
       "  157,\n",
       "  5592,\n",
       "  30,\n",
       "  29,\n",
       "  8351,\n",
       "  4,\n",
       "  6,\n",
       "  78,\n",
       "  31,\n",
       "  8,\n",
       "  203,\n",
       "  2,\n",
       "  3400,\n",
       "  6614,\n",
       "  7,\n",
       "  77,\n",
       "  5229,\n",
       "  37,\n",
       "  454,\n",
       "  0,\n",
       "  2,\n",
       "  937,\n",
       "  307,\n",
       "  46,\n",
       "  17,\n",
       "  66,\n",
       "  4845,\n",
       "  4,\n",
       "  2,\n",
       "  937,\n",
       "  307,\n",
       "  11,\n",
       "  17,\n",
       "  5362,\n",
       "  487,\n",
       "  3,\n",
       "  82,\n",
       "  4,\n",
       "  66,\n",
       "  4845,\n",
       "  173,\n",
       "  9,\n",
       "  28,\n",
       "  384,\n",
       "  66,\n",
       "  2407,\n",
       "  4,\n",
       "  6,\n",
       "  55,\n",
       "  618,\n",
       "  7,\n",
       "  5362,\n",
       "  615,\n",
       "  135,\n",
       "  2,\n",
       "  3307,\n",
       "  31,\n",
       "  56,\n",
       "  2,\n",
       "  23,\n",
       "  10,\n",
       "  37,\n",
       "  100,\n",
       "  94,\n",
       "  6702,\n",
       "  60,\n",
       "  834,\n",
       "  0,\n",
       "  4335,\n",
       "  13,\n",
       "  865,\n",
       "  3,\n",
       "  46,\n",
       "  11,\n",
       "  10,\n",
       "  4647,\n",
       "  4,\n",
       "  694,\n",
       "  4,\n",
       "  8,\n",
       "  2,\n",
       "  1253,\n",
       "  5657,\n",
       "  15,\n",
       "  266,\n",
       "  5,\n",
       "  325,\n",
       "  10526,\n",
       "  1698,\n",
       "  874,\n",
       "  279,\n",
       "  59,\n",
       "  105,\n",
       "  133,\n",
       "  3035,\n",
       "  1559,\n",
       "  3]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4b2e7c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.with_format(type='torch', columns=['ids', 'label'])\n",
    "valid_data = valid_data.with_format(type='torch', columns=['ids', 'label'])\n",
    "test_data = test_data.with_format(type='torch', columns=['ids', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c1782344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor(0),\n",
       " 'ids': tensor([   14,   627,    10,    37,   100,   125,    60,    11,    10,   361,\n",
       "           834,     2,     0,     0,     9, 12187,     6,  2407,  9694,     3,\n",
       "            46,    10,    66,  8861,     2, 16732,  3705,    13,  2360,     6,\n",
       "          4374,     7,    69,    77,     0, 13332,     3,   190,     4,    19,\n",
       "            41,  4597,    21,     0,  6574,    13,   822,     4,    14,    23,\n",
       "           962,  3426,     3,   265,  1267, 13332,   798,   266,   111,     0,\n",
       "          5592,     4,   157,  5592,    30,    29,  8351,     4,     6,    78,\n",
       "            31,     8,   203,     2,  3400,  6614,     7,    77,  5229,    37,\n",
       "           454,     0,     2,   937,   307,    46,    17,    66,  4845,     4,\n",
       "             2,   937,   307,    11,    17,  5362,   487,     3,    82,     4,\n",
       "            66,  4845,   173,     9,    28,   384,    66,  2407,     4,     6,\n",
       "            55,   618,     7,  5362,   615,   135,     2,  3307,    31,    56,\n",
       "             2,    23,    10,    37,   100,    94,  6702,    60,   834,     0,\n",
       "          4335,    13,   865,     3,    46,    11,    10,  4647,     4,   694,\n",
       "             4,     8,     2,  1253,  5657,    15,   266,     5,   325, 10526,\n",
       "          1698,   874,   279,    59,   105,   133,  3035,  1559,     3])}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "64a6432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch, pad_idex):\n",
    "    batch_ids = [i['ids'] for i in batch]\n",
    "    batch_ids = nn.utils.rnn.pad_sequence(batch_ids, \n",
    "                                          padding_value=pad_index, \n",
    "                                          batch_first=True)\n",
    "    batch_label = [i['label'] for i in batch]\n",
    "    batch_label = torch.stack(batch_label)\n",
    "    batch = {\n",
    "        'ids':batch_ids,\n",
    "        'label':batch_label\n",
    "    }\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1e53ed87",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512 \n",
    "colate = functools.partial(collate, pad_ind_dex=pad_index)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data,\n",
    "                                              batch_size=batch_size,\n",
    "                                              collate_fn=collate,\n",
    "                                              shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_data, \n",
    "                                               batch_size=batch_size, \n",
    "                                               collate_fn=collate)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, \n",
    "                                              batch_size=batch_size, \n",
    "                                              collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3790dfbc",
   "metadata": {},
   "source": [
    "# BIULD MODEL NBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edf7835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5b2d2db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBoW(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_dim,\n",
    "        output_dim,\n",
    "        pad_idx\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,\n",
    "                                     embedding_dim,\n",
    "                                     padding_idx=pad_index)\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        embedded = self.embedding(ids)\n",
    "        pooled = ebedded.mean(dim=1)\n",
    "        prediction = self.fc(pooled)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e0e9d64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 300\n",
    "output_dim = len(train_data.unique('label'))\n",
    "\n",
    "model = NBoW(vocab_size, embedding_dim, output_dim, pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "25def4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 6,458,402 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54cb8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/wiki.en.vec:  73% 4.84G/6.60G [13:12:42<4:42:54, 103kB/s]  "
     ]
    }
   ],
   "source": [
    "vectors = torchtext.vocab.FastText()\n",
    "hello_vector = vectors.get_vecs_by_tokens('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f5fb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "hello_vector.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24663976",
   "metadata": {},
   "outputs": [],
   "source": [
    "hello_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3efb44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding = vectors.get_vecs_by_tokens(vocab.get_itos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d72e029",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff02f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad17405",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cd8e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embedding.weight.data = pretrained_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8ec822",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6233d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e24df99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    epoch_accs = []\n",
    "\n",
    "    for batch in tqdm.tqdm(dataloader, desc='training...', file=sys.stdout):\n",
    "        ids = batch['ids'].to(device)\n",
    "        label = batch['label'].to(device)\n",
    "        prediction = model(ids)\n",
    "        loss = criterion(prediction, label)\n",
    "        accuracy = get_accuracy(prediction, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "        epoch_accs.append(accuracy.item())\n",
    "\n",
    "    return epoch_losses, epoch_accs\n",
    "\n",
    "\n",
    "def evaluate(dataloader, model, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_losses = []\n",
    "    epoch_accs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(dataloader, desc='evaluating...', file=sys.stdout):\n",
    "            ids = batch['ids'].to(device)\n",
    "            label = batch['label'].to(device)\n",
    "            prediction = model(ids)\n",
    "            loss = criterion(prediction, label)\n",
    "            accuracy = get_accuracy(prediction, label)\n",
    "            epoch_losses.append(loss.item())\n",
    "            epoch_accs.append(accuracy.item())\n",
    "\n",
    "    return epoch_losses, epoch_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc90501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(prediction, label):\n",
    "    batch_size, _ = prediction.shape\n",
    "    predicted_classes = prediction.argmax(dim=-1)\n",
    "    correct_predictions = predicted_classes.eq(label).sum()\n",
    "    accuracy = correct_predictions / batch_size\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baf14c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "valid_losses = []\n",
    "valid_accs = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    train_loss, train_acc = train(train_dataloader, model, criterion, optimizer, device)\n",
    "    valid_loss, valid_acc = evaluate(valid_dataloader, model, criterion, device)\n",
    "\n",
    "    train_losses.extend(train_loss)\n",
    "    train_accs.extend(train_acc)\n",
    "    valid_losses.extend(valid_loss)\n",
    "    valid_accs.extend(valid_acc)\n",
    "    \n",
    "    epoch_train_loss = np.mean(train_loss)\n",
    "    epoch_train_acc = np.mean(train_acc)\n",
    "    epoch_valid_loss = np.mean(valid_loss)\n",
    "    epoch_valid_acc = np.mean(valid_acc)\n",
    "    \n",
    "    if epoch_valid_loss < best_valid_loss:\n",
    "        best_valid_loss = epoch_valid_loss\n",
    "        torch.save(model.state_dict(), 'nbow.pt')\n",
    "    \n",
    "    print(f'epoch: {epoch+1}')\n",
    "    print(f'train_loss: {epoch_train_loss:.3f}, train_acc: {epoch_train_acc:.3f}')\n",
    "    print(f'valid_loss: {epoch_valid_loss:.3f}, valid_acc: {epoch_valid_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62370b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(train_losses, label='train loss')\n",
    "ax.plot(valid_losses, label='valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cd5002",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(train_accs, label='train accuracy')\n",
    "ax.plot(valid_accs, label='valid accuracy')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d239725",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('nbow.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(test_dataloader, model, criterion, device)\n",
    "\n",
    "epoch_test_loss = np.mean(test_loss)\n",
    "epoch_test_acc = np.mean(test_acc)\n",
    "\n",
    "print(f'test_loss: {epoch_test_loss:.3f}, test_acc: {epoch_test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bd1029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, tokenizer, vocab, device):\n",
    "    tokens = tokenizer(text)\n",
    "    ids = [vocab[t] for t in tokens]\n",
    "    tensor = torch.LongTensor(ids).unsqueeze(dim=0).to(device)\n",
    "    prediction = model(tensor).squeeze(dim=0)\n",
    "    probability = torch.softmax(prediction, dim=-1)\n",
    "    predicted_class = prediction.argmax(dim=-1).item()\n",
    "    predicted_probability = probability[predicted_class].item()\n",
    "    return predicted_class, predicted_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5042b6",
   "metadata": {},
   "source": [
    "# SENTENCES TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f6e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This film is terrible!\"\n",
    "\n",
    "predict_sentiment(text, model, tokenizer, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbbaf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This film is great!\"\n",
    "\n",
    "predict_sentiment(text, model, tokenizer, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333cf6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This film is not terrible, it's great!\"\n",
    "\n",
    "predict_sentiment(text, model, tokenizer, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842f7621",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This film is not great, it's terrible!\"\n",
    "\n",
    "predict_sentiment(text, model, tokenizer, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f7c44c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
